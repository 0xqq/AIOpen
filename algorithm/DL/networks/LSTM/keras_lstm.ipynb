{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]]\n",
      "[[0.]]\n",
      "[[0.]]\n",
      "[[0.]]\n",
      "[[0.]]\n",
      "[[7.]]\n",
      "[[0.]]\n",
      "[[0.]]\n",
      "[[6.]]\n",
      "h=[[6.]],c=[[6.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nhttp://blog.csdn.net/u011327333/article/details/78501054\\ninputs1 = Input(shape=(9, 3))\\nlstm1, state_h, state_c = LSTM(1, activation=None,return_sequences=True,return_state=True)(inputs1)\\nmodel = Model(inputs=inputs1, outputs=[lstm1, state_h, state_c])\\n\\nxxx = model.get_weights()\\nxxx[0]=np.array([[0,0,1,0],[100,100,0,0],[0,0,0,100]])\\nxxx[1]=np.array([[1,1,1,1]]) # \\xe8\\xbf\\x99\\xe4\\xb8\\xaa\\xe6\\x98\\xafh_t-1\\xe8\\xbe\\x93\\xe5\\x87\\xba\\xe7\\x9a\\x84\\xe5\\x8f\\x82\\xe6\\x95\\xb0\\xe7\\x9a\\x84w \\xe5\\x8f\\xaf\\xe4\\xbb\\xa5\\xe4\\xbd\\xbf\\xe7\\x94\\xa8 model.layers[0].re\\nxxx[2]=np.array([-10,10,0,-10])\\nmodel.set_weights(xxx)\\n\\nmodel.predict(x.reshape(-1,9,3))\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python2\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Jan  2 10:36:36 2018\n",
    "\n",
    "@author: tjian\n",
    "\"\"\"\n",
    "from keras.layers import LSTM\n",
    "from keras.models import Sequential\n",
    "\n",
    "\n",
    "\n",
    "# (unit + dim)*unit*4 + unit*4\n",
    "# (unit + dim + 1)*unit*4\n",
    "# 多lstm是将每个前面的所有输出再放到输入里来\n",
    "\n",
    "model = Sequential() \n",
    "model.add(LSTM(32,input_shape=(1,3)))\n",
    "# input_shape 1-> time step \n",
    "#             3-> data dim\n",
    "# 参数=4608   (32+3)*(32*4) + 32*4 4=控制z,z_in,z_out,z_forget\n",
    "\n",
    "model = Sequential() \n",
    "model.add(LSTM(32,input_shape=(1,1)))\n",
    "# input_shape 1-> time step \n",
    "#             1-> data dim\n",
    "# 参数=4532   (32+1)*(32*4) + 32*4  第一个4=控制z,z_in,z_out,z_forget\n",
    "# 第二个 4个bias\n",
    "\n",
    "# 测试李宏毅38讲 RNN的课件\n",
    "from keras.layers import LSTM\n",
    "from keras.models import Sequential\n",
    "import numpy as np\n",
    "\n",
    "x = np.array([[1,0,0],[3,1,0],[2,0,0],[4,1,0],[2,0,0],[1,0,1],[3,-1,0],[6,1,0],[1,0,1]])\n",
    "y = np.array([0,0,0,0,0,7,0,0,6])\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(1, activation=None, stateful=True, batch_input_shape=(1,1,3)))\n",
    "xxx = model.get_weights()\n",
    "xxx[0]=np.array([[0,0,1,0],[100,100,0,0],[0,0,0,100]])\n",
    "xxx[1]=np.array([[1,1,1,1]]) # 这个是h_t-1输出的参数的w 可以使用 model.layers[0].re\n",
    "xxx[2]=np.array([-10,10,0,-10])\n",
    "model.set_weights(xxx)\n",
    "\n",
    "\n",
    "print model.predict_on_batch(np.array([1,0,0]).reshape(-1,1,3))\n",
    "print model.predict_on_batch(np.array([3,1,0]).reshape(-1,1,3))\n",
    "print model.predict_on_batch(np.array([2,0,0]).reshape(-1,1,3))\n",
    "print model.predict_on_batch(np.array([4,1,0]).reshape(-1,1,3))\n",
    "print model.predict_on_batch(np.array([2,0,0]).reshape(-1,1,3))\n",
    "print model.predict_on_batch(np.array([1,0,1]).reshape(-1,1,3))\n",
    "print model.predict_on_batch(np.array([3,-1,0]).reshape(-1,1,3))\n",
    "print model.predict_on_batch(np.array([6,1,0]).reshape(-1,1,3))\n",
    "print model.predict_on_batch(np.array([1,0,1]).reshape(-1,1,3))\n",
    "\n",
    "\n",
    "from keras import backend as K\n",
    "K.eval(model.layers[0].states[0]) # h值\n",
    "K.eval(model.layers[0].states[1]) # c值\n",
    "\n",
    "print('h={},c={}'.format(K.eval(model.layers[0].states[0]),K.eval(model.layers[0].states[1])))\n",
    "\n",
    "model.reset_states() # 将 c值设置为0\n",
    "\n",
    "# 1. h=[[ 0.]],c=[[ 0.]]\n",
    "# 2. h=[[ 0.]],c=[[ 3.]]\n",
    "# 3. h=[[ 0.]],c=[[ 3.]]\n",
    "# 4. h=[[ 0.]],c=[[ 7.]]\n",
    "# 5. h=[[ 0.]],c=[[ 7.]]\n",
    "# 6. h=[[ 7.]],c=[[ 7.]]\n",
    "# 7. h=[[ 0.]],c=[[ 0.]]\n",
    "# 8. h=[[ 0.]],c=[[ 6.]]\n",
    "# 9. h=[[ 6.]],c=[[ 6.]]\n",
    "\n",
    "\n",
    "# 上面实现的另一种方法，更能很好地理解LSTM\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import LSTM\n",
    "import numpy as np\n",
    "\n",
    "x = np.array([[1,0,0],[3,1,0],[2,0,0],[4,1,0],[2,0,0],[1,0,1],[3,-1,0],[6,1,0],[1,0,1]])\n",
    "'''\n",
    "http://blog.csdn.net/u011327333/article/details/78501054\n",
    "inputs1 = Input(shape=(9, 3))\n",
    "lstm1, state_h, state_c = LSTM(1, activation=None,return_sequences=True,return_state=True)(inputs1)\n",
    "model = Model(inputs=inputs1, outputs=[lstm1, state_h, state_c])\n",
    "\n",
    "xxx = model.get_weights()\n",
    "xxx[0]=np.array([[0,0,1,0],[100,100,0,0],[0,0,0,100]])\n",
    "xxx[1]=np.array([[1,1,1,1]]) # 这个是h_t-1输出的参数的w 可以使用 model.layers[0].re\n",
    "xxx[2]=np.array([-10,10,0,-10])\n",
    "model.set_weights(xxx)\n",
    "\n",
    "model.predict(x.reshape(-1,9,3))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "理解LSTM："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.99018717,  1.0247939 , -0.37382555, -0.9029063 ]],\n",
       "       dtype=float32),\n",
       " array([[-0.53536123, -0.42826614, -0.68243545, -0.25349215]],\n",
       "       dtype=float32),\n",
       " array([0., 1., 0., 0.], dtype=float32)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import LSTM, Lambda,Input\n",
    "from keras.models import Model, Sequential\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(7) # 可重复weights随机值\n",
    "'''\n",
    " 0.39346123,  0.41240287, -0.38993686, -0.07691038]], dtype=float32),\n",
    " array([[-0.52859539, -0.79616123, -0.02723205,  0.29321092]], dtype=float32),\n",
    " array([ 0.,  1.,  0.,  0.], dtype=float32)]\n",
    "'''\n",
    "x = np.array([1,0,0])\n",
    "inputss = Input(shape=(3,1))\n",
    "lstm, state_h, state_c = LSTM(1,return_sequences=True,recurrent_activation='sigmoid',return_state=True)(inputss)\n",
    "#lstm = LSTM(1,return_sequences=True, recurrent_activation='sigmoid')(inputss) # LSTM缺省是hard_sigmoid\n",
    "# 研究：http://blog.csdn.net/silent56_th/article/details/73442391\n",
    "model = Model(inputs = inputss, outputs=[lstm,state_h,state_c])\n",
    "xxx = model.get_weights()\n",
    "xxx[2]=np.array([0.,0.,0.,0.])\n",
    "model.set_weights(xxx)\n",
    "x1=xxx[0]\n",
    "x2=xxx[1]\n",
    "xx=np.vstack((x2,x1))\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def tanh_h(x):\n",
    "    return (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))\n",
    "\n",
    "i_t = sigmoid(np.dot(input1.reshape(1,2), xx[:,0].reshape(2,1)))\n",
    "f_t = sigmoid(np.dot(input1.reshape(1,2), xx[:,1].reshape(2,1)))\n",
    "c_t = tanh_h(np.dot(input1.reshape(1,2), xx[:,2].reshape(2,1)))\n",
    "o_t = sigmoid(np.dot(input1.reshape(1,2), xx[:,3].reshape(2,1)))\n",
    "\n",
    "c_tt = i_t*c_t\n",
    "h_t = o_t*tanh_h(c_tt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xxx【0】是控制本cell的4个weihgt参数，依次是：\n",
    "$Gate_{in}, Gate_{foget},Contron_{in},Gate_{out}$ \n",
    "\n",
    "xxx【1】是控制上一个cell到本cell的4个weight参数，顺序同上\n",
    "\n",
    "xxx【2】是控制本cell的4个bias，顺序同上\n",
    "\n",
    "算法见https://www.jianshu.com/p/dcec3f07d3b5\n",
    "\n",
    "![avatar](img/lstm-1.png)\n",
    "![avatar](img/lstm-2.png)\n",
    "![avatar](img/lstm-3.png)\n",
    "\n",
    "如果是shape=（3，1）， timestep=3， input_dim=1:\n",
    "则需要学习：$[W_{ih},W_{ix}],[W_{fh},W_{fx}],[W_{ch},W_{cx}],[W_{oh},W_{ox}], 加上 bias=4，共8+4=12组参数 , 输入=[h_{t-1},x_t]$\n",
    "\n",
    "如果是shape=（1，3）， timestep=1， input_dim=3:\n",
    "则需要学习：$[W_{ih},W_{ix}-1,W_{ix}-2,W_{ix}-3],[W_{fh},W_{fx}-1,W_{fx}-2,W_{fx}-3],[W_{ch},W_{cx}-1,W_{cx}-2,W_{cx}-3],[W_{oh},W_{ox}-1,W_{ox}-2,W_{ox}-3], 加上 bias=4，共16+4=20组参数, 输入=[h_{t-1},x_t-1,x_t-2,x_t-3]$\n",
    "\n",
    "如果是shape=(3,1), timestep=3， input_dim=1, lstm=2:\n",
    "则需要学习：$[W_{ih}-1,W_{ih}-2,W_{ix}],[W_{fh}-1,W_{fh}-2,W_{fx}],[W_{ch}-1,W_{ch}-2,,W_{cx}],[W_{oh}-1,W_{oh}-2,W_{ox}], 加上 bias=4，一个LSTM有12+4=16组参数 , 两个有16x2=32组参数，输入=[h_{t-1}-1,h_{t-1}-2,x_t]$\n",
    "\n",
    "如果是shape=(1,3), timestep=1， input_dim=3, lstm=2: 则需要学习：\n",
    "则需要学习：$[W_{ih}-1,W_{ih}-2,W_{ix}-1,W_{ix}-2,W_{ix}-3],[W_{fh}-1,W_{fh}-2,W_{fx}-1,W_{fx}-2,W_{fx}-3],[W_{ch}-1,W_{ch}-2,W_{cx}-1,W_{cx}-2,W_{cx}-3],[W_{oh}-1,W_{oh}-2,W_{ox}-1,W_{ox}-2,W_{ox}-3], 加上 bias=4，一个LSTM有20+4=24组参数，两个LSTM则有 24x2=48组参数 输入=[h_{t-1}-1, h_{t-1}-2,x_t-1,x_t-2,x_t-3]$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs1 = Input(shape=(9, 3))\n",
    "lstm1 = LSTM(1, activation=None,return_sequences=True)(inputs1)\n",
    "model = Model(inputs=inputs1, outputs=[lstm1])\n",
    "\n",
    "model.compile(loss='mse',optimizer='adam')\n",
    "model.fit(x.reshape(-1,9,3),y.reshape(1,9,1),epochs=500,batch_size=1,verbose=2)\n",
    "\n",
    "\n",
    "# 参数个数：（x_times+x_dim)*unit + 4*unit^2 + 4*unit\n",
    "# 分别是： xx[0], xx[1], xx[2]\n",
    "# 即： kernal的weight， recurent的weight, 和 bias\n",
    "'''\n",
    "model = Sequential()\n",
    "model.add(LSTM(1,activation=None, return_sequences=True,input_shape=(9,3)))\n",
    "model.compile(loss='mse',optimizer='adam')\n",
    "model.fit(x.reshape(-1,9,3),y.reshape(1,9,1),epochs=500,batch_size=1,verbose=2)\n",
    "\n",
    "'''\n",
    "# 38讲RNN训练参数\n",
    "\n",
    "from keras.layers import LSTM\n",
    "from keras.models import Sequential\n",
    "import numpy as np\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "x = np.array([[1,0,0],[3,1,0],[2,0,0],[4,1,0],[2,0,0],[1,0,1],[3,-1,0],[6,1,0],[1,0,1]])\n",
    "y = np.array([0,0,0,0,0,7,0,0,6])\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(1, activation=None, stateful=True, batch_input_shape=(1,1,3)))\n",
    "\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
    "xx = x.reshape(-1,1,3)\n",
    "yy = y.reshape(-1,1)\n",
    "\n",
    "'''\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=2)\n",
    "\n",
    "for i in range(300):\n",
    "\tmodel.fit(xx, yy, epochs=1, batch_size=1, shuffle=False, callbacks=[early_stopping])\n",
    "\tmodel.reset_states()\n",
    "    \n",
    "# model.fit(xx,yy,batch_size=1,epochs=5032,shuffle=False)\n",
    "'''\n",
    "\n",
    "for i in range(1000):\n",
    "    for x_i,y_i in zip(x,y):\n",
    "        x_i = x_i.reshape(-1,1,3)\n",
    "        y_i = y_i.reshape(1,1)\n",
    "        model.train_on_batch(x_i, y_i)\n",
    "    model.reset_states()\n",
    "    if(i%50==0):\n",
    "        loss_and_metric = model.evaluate(xx, yy, batch_size=1)    \n",
    "        print loss_and_metric\n",
    "\n",
    "# loss_and_metrics = model.evaluate(xx, yy, batch_size=1)\n",
    "# classes = model.predict_classes(xx, batch_size=1)\n",
    "# pred = model.predict_proba(xx, batch_size=1)\n",
    "        \n",
    "\n",
    "# 例子研究\n",
    "        \n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import LSTM\n",
    "from numpy import array\n",
    "# define model\n",
    "inputs1 = Input(shape=(3, 1))\n",
    "lstm1, state_h, state_c = LSTM(1, return_state=True)(inputs1)\n",
    "model = Model(inputs=inputs1, outputs=[lstm1, state_h, state_c])\n",
    "# define input data\n",
    "data = array([0.1, 0.2, 0.3]).reshape((1,3,1))\n",
    "# make and show prediction\n",
    "print(model.predict(data))\n",
    "\n",
    "\n",
    "# 例子研究\n",
    "# http://www.guanggua.com/question/43237170-keras-stateful-lstm.html\n",
    "\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.models import Sequential\n",
    "import numpy as np\n",
    "\n",
    "nb_samples = 100000\n",
    "X = np.random.randn(nb_samples)\n",
    "Y = X[1:]\n",
    "X = X[:-1]\n",
    "X = X.reshape((len(Y), 1, 1))\n",
    "Y = Y.reshape((len(Y), 1))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(batch_input_shape=(1, 1, 1),\n",
    "               units =10,\n",
    "               activation='tanh', stateful=True\n",
    "          )\n",
    "    )\n",
    "model.add(Dense(units=1, activation=None))\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "for epoch in range(10000):\n",
    "    model.reset_states()\n",
    "    train_loss = 0\n",
    "    for i in range(Y.shape[0]):\n",
    "        train_loss += model.train_on_batch(X[i:i+1],\n",
    "                         Y[i:i+1],\n",
    "                         )\n",
    "    print '# epoch', epoch, '  loss ', train_loss/float(Y.shape[0])\n",
    "    \n",
    "    \n",
    "    \n",
    "# 研究SimpleRNN, SimpleRUUCell, RNN\n",
    "from keras.layers import RNN,SimpleRNN,SimpleRNNCell\n",
    "from keras.models import Sequential\n",
    "import numpy as np\n",
    "\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(1, input_shape=(3,1), activation=None))\n",
    "\n",
    "xx = model.get_weights()\n",
    "xx[0]=np.array([[1.]]) # x输入的W\n",
    "xx[1]=np.array([[2.]]) # cell 的W\n",
    "xx[2]=np.array([0]) # x输入的bias\n",
    "\n",
    "model.set_weights(xx)\n",
    "\n",
    "model.predict(np.array([1,1,1]).reshape(-1,3,1)) # =7\n",
    "# 1-> cell=1, out=1\n",
    "# 2-> cell=w*前cell=2*1 = 2 + x*1 = 3, out=3\n",
    "# 3-> cell=2*3+1=7, out = 7\n",
    "\n",
    "model.predict(np.array([1,2,3]).reshape(-1,3,1)) # =15\n",
    "# 1-> cell=1, out=1\n",
    "# 2-> cell=w*前cell=2*1  + 1*2 = 4, out=4\n",
    "# 3-> cell=2*4+1*3=11, out = 11\n",
    "\n",
    "xx[0]=np.array([[2.]]) # x输入的W\n",
    "xx[1]=np.array([[1.]]) # cell 的W\n",
    "xx[2]=np.array([0]) # x输入的bias\n",
    "model.set_weights(xx)\n",
    "\n",
    "model.predict(np.array([1,1,1]).reshape(-1,3,1)) # =7\n",
    "# 1-> cell=1*0+2*1, out=2\n",
    "# 2-> cell=w*前cell=1*2+ 2*1 = 4, out=4\n",
    "# 3-> cell=1*4+2*1=6, out = 6\n",
    "\n",
    "model.predict(np.array([1,2,3]).reshape(-1,3,1)) # =11\n",
    "# 1-> cell=1*0+2*1, out=2\n",
    "# 2-> cell=w*前cell=1*2+ 2*2 = 6, out=6\n",
    "# 3-> cell=1*6+2*3=12, out = 12\n",
    "\n",
    "\n",
    "# http://blog.csdn.net/u013518890/article/details/74938758\n",
    "# LSTM with Variable Length Input Sequences to One Character Output\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from theano.tensor.shared_randomstreams import RandomStreams\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "# define the raw dataset\n",
    "alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "# create mapping of characters to integers (0-25) and the reverse\n",
    "char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "num_inputs = 1000\n",
    "max_len = 5\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(num_inputs):\n",
    "    start = numpy.random.randint(len(alphabet)-2)\n",
    "    end = numpy.random.randint(start, min(start+max_len,len(alphabet)-1))\n",
    "    sequence_in = alphabet[start:end+1]\n",
    "    sequence_out = alphabet[end + 1]\n",
    "    dataX.append([char_to_int[char] for char in sequence_in])\n",
    "    dataY.append(char_to_int[sequence_out])\n",
    "    print sequence_in, '->', sequence_out\n",
    "# convert list of lists to array and pad sequences if needed\n",
    "X = pad_sequences(dataX, maxlen=max_len, dtype='float32')\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(X, (X.shape[0], max_len, 1))\n",
    "# normalize\n",
    "X = X / float(len(alphabet))\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "# create and fit the model\n",
    "batch_size = 1\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, input_shape=(X.shape[1], 1)))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=500, batch_size=batch_size, verbose=2)\n",
    "# summarize performance of the model\n",
    "scores = model.evaluate(X, y, verbose=0)\n",
    "print(\"Model Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "# demonstrate some model predictions\n",
    "for i in range(20):\n",
    "    pattern_index = numpy.random.randint(len(dataX))\n",
    "    pattern = dataX[pattern_index]\n",
    "    x = pad_sequences([pattern], maxlen=max_len, dtype='float32')\n",
    "    x = numpy.reshape(x, (1, max_len, 1))\n",
    "    x = x / float(len(alphabet))\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    print seq_in, \"->\", result\n",
    "    \n",
    "    \n",
    "# lstm mnist实现：97%\n",
    "\n",
    "import cPickle as pickle\n",
    "import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def read_data(data_file):  \n",
    "    import gzip  \n",
    "    f = gzip.open(data_file, \"rb\")  \n",
    "    train, val, test = pickle.load(f)  \n",
    "    f.close()  \n",
    "    train_x = train[0]  \n",
    "    train_y = train[1]  \n",
    "    test_x = test[0]  \n",
    "    test_y = test[1]  \n",
    "    return train_x, train_y, test_x, test_y\n",
    "\n",
    "train_x, train_y, test_x, test_y = read_data('../dataset/mnist.pkl.gz')\n",
    "x_train = train_x.reshape(-1,28,28)\n",
    "x_test = test_x.reshape(-1,28,28)\n",
    "\n",
    "y_train = np_utils.to_categorical(train_y,10)\n",
    "y_test = np_utils.to_categorical(test_y,10)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(32,input_shape=(28,28)))\n",
    "model.add(Dense(10,activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train,y_train)\n",
    "model.evaluate(x_test,y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
